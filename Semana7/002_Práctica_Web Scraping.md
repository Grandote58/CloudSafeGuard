

![M1](https://github.com/Grandote58/CloudSafeGuard/blob/main/Recursos/Recurso%201%402Menbrete1.png)

# **ğŸğŸ“Š PrÃ¡ctica de Web Scraping para AnÃ¡lisis de Precios de Productos de la Competencia **

## **ğŸ“Œ IntroducciÃ³n**

Imagina que trabajas para una empresa de venta de productos comestibles y necesitas conocer los precios de un competidor en lÃ­nea. En esta prÃ¡ctica, aprenderÃ¡s a realizar **web scraping** en una tienda de alimentos en lÃ­nea para extraer informaciÃ³n sobre productos y precios.

Para este ejercicio, utilizaremos la web **https://scrapeme.live/shop/**, que estÃ¡ diseÃ±ada para la prÃ¡ctica de scraping y simula una tienda de comestibles.

------

## **ğŸ“Œ Objetivos**

ğŸ”¹ Realizar web scraping sobre una tienda de alimentos en lÃ­nea.
ğŸ”¹ Extraer informaciÃ³n relevante: **nombre del producto, precio e imagen**.
ğŸ”¹ Identificar los selectores CSS o XPath para acceder a los datos.
ğŸ”¹ Guardar los datos en un archivo CSV para su anÃ¡lisis.

------

## **ğŸ“Œ Paso 1: Configurar Google Colab**

1. Abre Google Colab y crea un nuevo cuaderno.
2. Cambia el nombre del archivo a **"Web_Scraping_Competencia"**.

------

## **ğŸ“Œ Paso 2: Instalar y Importar LibrerÃ­as**

Ejecuta la siguiente celda para instalar y cargar las librerÃ­as necesarias:

```python
# InstalaciÃ³n de librerÃ­as necesarias
!pip install requests beautifulsoup4 pandas lxml

# Importar las librerÃ­as necesarias
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time  # Para evitar ser bloqueados por la web
```

ğŸ”¹ **ExplicaciÃ³n de librerÃ­as:**

- `requests`: Para realizar solicitudes HTTP y obtener el contenido de la pÃ¡gina.
- `BeautifulSoup`: Para analizar y extraer datos del HTML.
- `pandas`: Para organizar y guardar los datos en un archivo CSV.
- `lxml`: Motor de anÃ¡lisis de HTML mÃ¡s eficiente.

------

## **ğŸ“Œ Paso 3: Inspeccionar la Web y Obtener la URL**

Utilizaremos la pÃ¡gina **Scrapeme Shop**, una tienda ficticia de productos de supermercado.

**ğŸ” ExploraciÃ³n del sitio:**

1. Abre la web en tu navegador.
2. Haz clic derecho en cualquier producto y selecciona **"Inspeccionar"**.
3. Observa la estructura HTML y busca los elementos relevantes como:
   - Nombre del producto
   - Precio
   - Imagen

------

## **ğŸ“Œ Paso 4: Identificar los Selectores CSS o XPath**

Tras analizar el cÃ³digo HTML, identificamos los siguientes selectores CSS:

- **Nombre del producto**: `h2.woocommerce-loop-product__title`
- **Precio**: `span.woocommerce-Price-amount`
- **Imagen del producto**: `img.attachment-woocommerce_thumbnail`

TambiÃ©n podemos usar XPath (alternativa a CSS Selectors):

- **Nombre del producto**: `//h2[@class="woocommerce-loop-product__title"]`
- **Precio**: `//span[@class="woocommerce-Price-amount amount"]`
- **Imagen**: `//img[@class="attachment-woocommerce_thumbnail size-woocommerce_thumbnail"]`

------

## **ğŸ“Œ Paso 5: Extraer los Datos de una PÃ¡gina**

Ahora haremos una solicitud HTTP para obtener la pÃ¡gina y extraer los datos.

```python
# URL de la tienda
url = "https://scrapeme.live/shop/"

# Hacer la solicitud HTTP
response = requests.get(url)

# Verificar si la solicitud fue exitosa
if response.status_code == 200:
    print("ConexiÃ³n exitosa")
else:
    print("Error en la conexiÃ³n", response.status_code)

# Analizar el contenido HTML con BeautifulSoup
soup = BeautifulSoup(response.text, "lxml")

# Encontrar todos los productos en la pÃ¡gina
productos = soup.find_all("li", class_="product")

# Lista para almacenar los datos
datos_productos = []

# Extraer datos de cada producto
for producto in productos:
    nombre = producto.find("h2", class_="woocommerce-loop-product__title").text
    precio = producto.find("span", class_="woocommerce-Price-amount").text
    imagen = producto.find("img", class_="attachment-woocommerce_thumbnail")["src"]

    # Guardar en la lista
    datos_productos.append([nombre, precio, imagen])

# Mostrar los primeros 5 resultados
for i in range(5):
    print(datos_productos[i])
```

âœ… **VerificaciÃ³n**: DeberÃ­as ver una lista con el nombre, precio y enlace de la imagen del producto.

------

## **ğŸ“Œ Paso 6: Guardar los Datos en un DataFrame**

Ahora organizamos los datos en una tabla usando `pandas`.

```python
# Convertir los datos en un DataFrame de pandas
df = pd.DataFrame(datos_productos, columns=["Nombre", "Precio", "Imagen"])

# Mostrar los primeros registros
df.head()
```

ğŸ“Š **ExplicaciÃ³n**: `pandas` estructura los datos en un formato tabular para su anÃ¡lisis.

------

## **ğŸ“Œ Paso 7: Almacenar los Datos en un Archivo CSV**

```python
# Guardar el DataFrame en un archivo CSV
df.to_csv("precios_competencia.csv", index=False, encoding="utf-8")

print("Archivo CSV guardado exitosamente.")
```

âœ… **Resultado esperado**: Se genera un archivo `precios_competencia.csv` con los datos extraÃ­dos.

------

## **ğŸ“Œ Paso 8: Descargar el Archivo CSV**

Para descargar el archivo desde Google Colab a tu computadora:

```python
from google.colab import files
files.download("precios_competencia.csv")
```

ğŸ“¥ **ExplicaciÃ³n**: Permite descargar el archivo desde Google Colab a tu equipo.

------

## **ğŸ“Œ Paso 9: Scraping de MÃºltiples PÃ¡ginas**

Si la web tiene paginaciÃ³n, podemos recorrer varias pÃ¡ginas automÃ¡ticamente.

```python
# URL base con paginaciÃ³n
base_url = "https://scrapeme.live/shop/page/{}/"

# Lista para almacenar todos los productos
todos_los_productos = []

# Extraer datos de las primeras 3 pÃ¡ginas
for pagina in range(1, 4):
    print(f"Scraping pÃ¡gina {pagina}...")

    # Hacer la solicitud HTTP
    response = requests.get(base_url.format(pagina))

    # Verificar que la pÃ¡gina existe
    if response.status_code != 200:
        break

    # Analizar el contenido
    soup = BeautifulSoup(response.text, "lxml")

    # Encontrar los productos en la pÃ¡gina
    productos = soup.find_all("li", class_="product")

    # Extraer los datos
    for producto in productos:
        nombre = producto.find("h2", class_="woocommerce-loop-product__title").text
        precio = producto.find("span", class_="woocommerce-Price-amount").text
        imagen = producto.find("img", class_="attachment-woocommerce_thumbnail")["src"]

        todos_los_productos.append([nombre, precio, imagen])

    # Pausa de 2 segundos para evitar sobrecarga del servidor
    time.sleep(2)

# Crear DataFrame con todos los productos
df_todos = pd.DataFrame(todos_los_productos, columns=["Nombre", "Precio", "Imagen"])

# Guardar en CSV
df_todos.to_csv("precios_competencia_multiples_paginas.csv", index=False, encoding="utf-8")

print("Scraping completado y datos guardados.")
```

âœ… **VerificaciÃ³n**: Se genera un archivo con datos de mÃºltiples pÃ¡ginas.

------

## **ğŸ“Œ ConclusiÃ³n**

Has aprendido a: 

âœ… Inspeccionar un sitio web y extraer informaciÃ³n relevante.
âœ… Identificar selectores CSS y XPath para scraping.
âœ… Extraer y organizar datos de productos de un competidor.
âœ… Guardar los datos en un archivo CSV.
âœ… Automatizar la extracciÃ³n de datos de mÃºltiples pÃ¡ginas.

ğŸ¯ **Siguientes pasos**: Intenta aplicar estas tÃ©cnicas a otras tiendas en lÃ­nea de productos comestibles.

------

ğŸ“¢ **Â¡Felicidades! Ahora tienes una prÃ¡ctica completa para monitorear los precios de la competencia en lÃ­nea ğŸš€**



![M2](https://github.com/Grandote58/CloudSafeGuard/blob/main/Recursos/Recurso%203%402Menbrete2.png)