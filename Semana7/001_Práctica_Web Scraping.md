![M1](https://github.com/Grandote58/CloudSafeGuard/blob/main/Recursos/Recurso%201%402Menbrete1.png)

# **Pr√°ctica de Web Scraping en Google Colab** üìäüåê

## **Objetivo**

Aprender a extraer informaci√≥n de p√°ginas web de manera autom√°tica utilizando Python en Google Colab. Se utilizar√°n librer√≠as como `requests`, `BeautifulSoup`, y `pandas`, y se guardar√°n los datos en un archivo CSV.

## **üìå Paso 1: Configurar Google Colab**

1. Abre Google Colab y crea un nuevo cuaderno.
2. Cambia el nombre del archivo a **"Web_Scraping_Practica"**.

------

## **üìå Paso 2: Instalar y Importar Librer√≠as**

Ejecuta el siguiente c√≥digo en una celda de Google Colab para instalar y cargar las librer√≠as necesarias:

```python
# Instalaci√≥n de librer√≠as (si no est√°n instaladas por defecto)
!pip install requests beautifulsoup4 pandas lxml

# Importar las librer√≠as necesarias
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time  # Para evitar ser bloqueados por la web al hacer m√∫ltiples solicitudes
```

üîπ **Explicaci√≥n de las librer√≠as:**

- `requests`: Permite hacer solicitudes HTTP para obtener el contenido de una p√°gina web.
- `BeautifulSoup`: Analiza el HTML y extrae la informaci√≥n deseada.
- `pandas`: Organiza y guarda los datos en archivos.
- `lxml`: Motor de an√°lisis de HTML para mejorar la compatibilidad con `BeautifulSoup`.

------

## **üìå Paso 3: Elegir una P√°gina Web para Scraping**

Para esta pr√°ctica, utilizaremos **https://books.toscrape.com/**, una p√°gina dise√±ada para pruebas de scraping, donde extraeremos informaci√≥n sobre libros.

------

## **üìå Paso 4: Hacer una Solicitud HTTP a la Web**

Ejecuta el siguiente c√≥digo para obtener el contenido de la p√°gina:

```python
# URL de la p√°gina a analizar
url = "https://books.toscrape.com/"

# Hacer la solicitud HTTP
response = requests.get(url)

# Verificar si la solicitud fue exitosa
if response.status_code == 200:
    print("Conexi√≥n exitosa")
else:
    print("Error en la conexi√≥n", response.status_code)

# Mostrar los primeros 500 caracteres del HTML obtenido
print(response.text[:500])
```

‚úÖ **Verificaci√≥n**: Si el c√≥digo devuelve "Conexi√≥n exitosa", la p√°gina est√° lista para el scraping.

------

## **üìå Paso 5: Analizar el HTML con BeautifulSoup**

Ejecuta el siguiente c√≥digo para analizar la estructura HTML de la p√°gina:

```python
# Analizar el contenido HTML con BeautifulSoup
soup = BeautifulSoup(response.text, "lxml")

# Ver la estructura del HTML
print(soup.prettify()[:1000])  # Imprime los primeros 1000 caracteres formateados
```

üîç **Explicaci√≥n**: `prettify()` formatea el HTML para que sea m√°s legible.

------

## **üìå Paso 6: Extraer Informaci√≥n Espec√≠fica**

Buscaremos los t√≠tulos de los libros, sus precios y su disponibilidad.

```python
# Encontrar todos los contenedores de libros
books = soup.find_all("article", class_="product_pod")

# Lista para almacenar los datos
book_data = []

# Extraer datos de cada libro
for book in books:
    title = book.h3.a["title"]  # Obtener el t√≠tulo del libro
    price = book.find("p", class_="price_color").text  # Obtener el precio
    availability = book.find("p", class_="instock availability").text.strip()  # Obtener disponibilidad
    
    # Guardar en la lista
    book_data.append([title, price, availability])

# Mostrar los primeros 5 resultados
for i in range(5):
    print(book_data[i])
```

‚úÖ **Verificaci√≥n**: Deber√≠as ver una lista con los t√≠tulos de los libros, precios y disponibilidad.

------

## **üìå Paso 7: Guardar los Datos en un DataFrame**

```python
# Convertir los datos en un DataFrame de pandas
df = pd.DataFrame(book_data, columns=["T√≠tulo", "Precio", "Disponibilidad"])

# Mostrar los primeros registros
df.head()
```

üìä **Explicaci√≥n**: `pandas` organiza la informaci√≥n en formato tabular para facilitar su an√°lisis.

------

## **üìå Paso 8: Almacenar los Datos en un Archivo CSV**

```python
# Guardar el DataFrame en un archivo CSV
df.to_csv("libros_scraping.csv", index=False, encoding="utf-8")

print("Archivo CSV guardado exitosamente.")
```

‚úÖ **Resultado esperado**: Se genera un archivo `libros_scraping.csv` con los datos extra√≠dos.

------

## **üìå Paso 9: Descargar el Archivo CSV en Google Colab**

Para descargar el archivo desde Google Colab a tu computadora:

```python
from google.colab import files
files.download("libros_scraping.csv")
```

üì• **Explicaci√≥n**: Esta funci√≥n permite descargar el archivo de Colab a tu equipo.

------

## **üìå Paso 10: Mejorar el Scraping con Navegaci√≥n entre P√°ginas**

Para extraer datos de varias p√°ginas, podemos recorrerlas autom√°ticamente:

```python
# URL base de la web
base_url = "https://books.toscrape.com/catalogue/page-{}.html"

# Lista para almacenar los datos
all_books = []

# Extraer datos de las primeras 5 p√°ginas
for page in range(1, 6):  # Modifica este n√∫mero seg√∫n las p√°ginas a extraer
    print(f"Scraping p√°gina {page}...")
    
    # Hacer la solicitud HTTP
    response = requests.get(base_url.format(page))
    
    # Verificar que la p√°gina existe
    if response.status_code != 200:
        break

    # Analizar el contenido
    soup = BeautifulSoup(response.text, "lxml")
    
    # Encontrar los libros en la p√°gina
    books = soup.find_all("article", class_="product_pod")

    # Extraer los datos
    for book in books:
        title = book.h3.a["title"]
        price = book.find("p", class_="price_color").text
        availability = book.find("p", class_="instock availability").text.strip()
        
        all_books.append([title, price, availability])

    # Esperar 2 segundos entre p√°ginas para no sobrecargar el servidor
    time.sleep(2)

# Crear DataFrame con todos los datos
df_all = pd.DataFrame(all_books, columns=["T√≠tulo", "Precio", "Disponibilidad"])

# Guardar en CSV
df_all.to_csv("libros_multiples_paginas.csv", index=False, encoding="utf-8")

print("Scraping completado y datos guardados.")
```

------

## **üìå Conclusi√≥n**

Has aprendido a: 

‚úÖ Instalar y usar `requests` y `BeautifulSoup`.

‚úÖ Extraer informaci√≥n espec√≠fica de una p√°gina web.

‚úÖ Guardar los datos en un archivo CSV.

‚úÖ Recorrer m√∫ltiples p√°ginas autom√°ticamente.



![M2](https://github.com/Grandote58/CloudSafeGuard/blob/main/Recursos/Recurso%203%402Menbrete2.png)